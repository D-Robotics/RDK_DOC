"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[8219],{59762:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>l,frontMatter:()=>a,metadata:()=>d,toc:()=>c});var o=n(74848),i=n(28453);const a={},r="7.4.3.4 Advanced Guide",d={id:"Advanced_development/toolchain_development/expert/expert",title:"7.4.3.4 Advanced Guide",description:"Quantization refers to the technique of performing calculations and storing tensors with a bit width lower than floating-point precision. Quantized models use integers instead of floating-point values to perform partial or complete operations on tensors. Compared to typical FP32 models, horizonpluginpytorch supports INT8 quantization, which reduces the model size by 4 times and reduces the memory bandwidth requirement by 4 times. Hardware support for INT8 calculations is usually 2 to 4 times faster than FP32 calculations. Quantization is mainly a technique for accelerating inference, and quantization operations only support forward calculations.",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/07_Advanced_development/04_toolchain_development/expert/expert.md",sourceDirName:"07_Advanced_development/04_toolchain_development/expert",slug:"/Advanced_development/toolchain_development/expert/",permalink:"/rdk_doc/en/Advanced_development/toolchain_development/expert/",draft:!1,unlisted:!1,editUrl:"https://github.com/D-Robotics/rdk_doc/blob/main/docs/07_Advanced_development/04_toolchain_development/expert/expert.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"On-board Model Application Development Guide",permalink:"/rdk_doc/en/Advanced_development/toolchain_development/intermediate/runtime_sample"},next:{title:"Environment Dependency",permalink:"/rdk_doc/en/Advanced_development/toolchain_development/expert/environment_config"}},s={},c=[];function p(e){const t={h1:"h1",p:"p",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.h1,{id:"7434-advanced-guide",children:"7.4.3.4 Advanced Guide"}),"\n",(0,o.jsx)(t.p,{children:"Quantization refers to the technique of performing calculations and storing tensors with a bit width lower than floating-point precision. Quantized models use integers instead of floating-point values to perform partial or complete operations on tensors. Compared to typical FP32 models, horizon_plugin_pytorch supports INT8 quantization, which reduces the model size by 4 times and reduces the memory bandwidth requirement by 4 times. Hardware support for INT8 calculations is usually 2 to 4 times faster than FP32 calculations. Quantization is mainly a technique for accelerating inference, and quantization operations only support forward calculations."}),"\n",(0,o.jsx)(t.p,{children:"horizon_plugin_pytorch provides quantization operations adapted to BPU, supporting quantization-aware training. This training uses pseudo-quantization modules to model quantization errors in forward calculations and backpropagation. Please note that the entire computation process of quantization-aware training is performed using floating-point operations. At the end of quantization-aware training, horizon_plugin_pytorch provides conversion functions to transform the trained model into a fixed-point model, using a more compact model representation and high-performance vectorized operations on BPU."}),"\n",(0,o.jsx)(t.p,{children:"This chapter provides a detailed introduction to the quantization training tool of horizon_plugin_pytorch developed based on PyTorch."})]})}function l(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>r,x:()=>d});var o=n(96540);const i={},a=o.createContext(i);function r(e){const t=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),o.createElement(a.Provider,{value:t},e.children)}}}]);