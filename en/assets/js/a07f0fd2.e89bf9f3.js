"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[847],{25463:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>c});var s=i(74848),t=i(28453);const o={sidebar_position:2},r="3.4.2  Reference Example \uff08C++\uff09",a={id:"Basic_Application/multi_media/cdev_demo",title:"3.4.2  Reference Example \uff08C++\uff09",description:"This chapter introduces various functional examples for multimedia library development, including camera image capture, video encoding and decoding, video display, algorithm inference, and more.",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/03_Basic_Application/04_multi_media/cdev_demo.md",sourceDirName:"03_Basic_Application/04_multi_media",slug:"/Basic_Application/multi_media/cdev_demo",permalink:"/rdk_doc/en/Basic_Application/multi_media/cdev_demo",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/03_Basic_Application/04_multi_media/cdev_demo.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"3.4.1 Reference Examples (python)",permalink:"/rdk_doc/en/Basic_Application/multi_media/pydev_vio_demo"},next:{title:"3.4.3 RDK X3/X5\u591a\u5a92\u4f53\u63a5\u53e3\u8bf4\u660e",permalink:"/rdk_doc/en/Basic_Application/multi_media/multi_media_api/pydev_multimedia_api_x3/"}},d={},c=[{value:"Camera Image Capture and Display",id:"camera-image-capture-and-display",level:2},{value:"Camera Image Local Save (RDK X3)",id:"camera-image-local-save-rdk-x3",level:2},{value:"Camera Image Local Saving (RDK Ultra)",id:"camera-image-local-saving-rdk-ultra",level:2},{value:"Camera Image Capture and Encoding",id:"camera-image-capture-and-encoding",level:2},{value:"Video file decoding and displaying",id:"video-file-decoding-and-displaying",level:2},{value:"RTSP Streaming Decode",id:"rtsp-streaming-decode",level:2},{value:"VPS Scaling Example",id:"vps-scaling-example",level:2},{value:"Object Detection Algorithm - FCOS",id:"object-detection-algorithm---fcos",level:2},{value:"Object Detection Algorithm - YOLOv5",id:"object-detection-algorithm---yolov5",level:2}];function l(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"342--reference-example-c",children:"3.4.2  Reference Example \uff08C++\uff09"}),"\n",(0,s.jsx)(n.p,{children:"This chapter introduces various functional examples for multimedia library development, including camera image capture, video encoding and decoding, video display, algorithm inference, and more."}),"\n",(0,s.jsx)(n.h2,{id:"camera-image-capture-and-display",children:"Camera Image Capture and Display"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"vio2display"})," example implements the functionality of capturing images from a ",(0,s.jsx)(n.code,{children:"MIPI"})," camera and outputting them via the ",(0,s.jsx)(n.code,{children:"HDMI"})," interface for preview on a display. The flowchart of the example is shown below:\n",(0,s.jsx)(n.img,{alt:"image-vio_to_display",src:i(96600).A+"",width:"1280",height:"925"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Environment Setup:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["With the development board powered off, connect the ",(0,s.jsx)(n.code,{children:"MIPI"})," camera to the development board. Refer to the ",(0,s.jsx)(n.a,{href:"../installation/hardware_interface#mipi_port",children:"MIPI camera connection tutorial"})," for the connection method."]}),"\n",(0,s.jsx)(n.li,{children:"Connect the development board to the display via an HDMI cable."}),"\n",(0,s.jsx)(n.li,{children:"Power on the development board and log in through the command line."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Running Instructions:"}),"\nThe example code is provided in source code form and needs to be compiled and run using the ",(0,s.jsx)(n.code,{children:"make"})," command. Follow these steps:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:~$ cd /app/cdev_demo/vio2display\nsunrise@ubuntu:/app/cdev_demo/vio2display$ sudo make\nsunrise@ubuntu:/app/cdev_demo/vio2display$ sudo ./vio2display -w 1920 -h 1080\n"})}),"\n",(0,s.jsx)(n.p,{children:"Parameters:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"-w: sensor output width"}),"\n",(0,s.jsx)(n.li,{children:"-h: sensor output height"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Results:"}),"\nAfter the program runs successfully, the development board will output the real-time image captured by the ",(0,s.jsx)(n.code,{children:"MIPI"})," camera to the display. The running log is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:/tmp/nfs/sp_cdev/cdev_demo/vio2display$ ./vio2display -w 1920 -h 1080\ndisp_w=1920, disp_h=1080\n2023/03/28 02:08:03.359 !INFO [x3_cam_init_param][0099]Enable mipi host0 mclk\n2023/03/28 02:08:03.359 !INFO [x3_cam_init_param][0099]Enable mipi host1 mclk\nCamera: gpio_num=114, active=low, i2c_bus=3, mipi_host=0\nCamera: gpio_num=114, active=low, i2c_bus=1, mipi_host=1\nCamera: gpio_num=114, active=low, i2c_bus=0, mipi_host=2\nCamera 0:\n      enable: 1\n      i2c_bus: 3\n      mipi_host: 0\nCamera 1:\n      enable: 1\n      i2c_bus: 1\n      mipi_host: 1\nCamera 2:\n      enable: 1\n      i2c_bus: 0\n      mipi_host: 2\ncmd=i2ctransfer -y -f 3 w2@0x10 0x0 0x0 r1 2>&1, result=0x02\n\nFound sensor:imx219 on i2c bus 3, use mipi host 0\nSetting VPS channel-2: src_w:1920, src_h:1080; dst_w:1920, dst_h:1080;\nSetting VPS channel-1: src_w:1920, src_h:1080; dst_w:1920, dst_h:1080;\nsp_open_camera success!\nlibiar: hb_disp_set_timing done!\n\nPress 'q' to Exit !\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"camera-image-local-save-rdk-x3",children:"Camera Image Local Save (RDK X3)"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"vio_capture"})," example in this document realizes the function of capturing images from a ",(0,s.jsx)(n.code,{children:"MIPI"})," camera and saving the images locally in both ",(0,s.jsx)(n.code,{children:"RAW"})," and ",(0,s.jsx)(n.code,{children:"YUV"})," formats. The flowchart of the example is as follows:\n",(0,s.jsx)(n.img,{alt:"image-capture",src:i(47695).A+"",width:"1280",height:"1055"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Preparation:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Connect the ",(0,s.jsx)(n.code,{children:"MIPI"})," camera to the development board while the development board is powered off. For the connection method, see ",(0,s.jsx)(n.a,{href:"../../01_Quick_start/hardware_introduction.md",children:"MIPI Camera Connection Guide"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Connect the development board and the monitor with an HDMI cable."}),"\n",(0,s.jsx)(n.li,{children:"Power on the development board and log in through the command line."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Running the Example:"}),"\nThe example code is provided in source code form. After compiling the code using the ",(0,s.jsx)(n.code,{children:"make"})," command, run the example as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:~$ cd /app/cdev_demo/vio_capture/\nsunrise@ubuntu:/app/cdev_demo/vio_capture$ sudo make\nsunrise@ubuntu:/app/cdev_demo/vio_capture$ sudo ./capture -b 12 -c 10 -h 1080 -w 1920\n"})}),"\n",(0,s.jsx)(n.p,{children:"Parameter description:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"-b: bit number of the RAW image, IMX477: 12, others: 10"}),"\n",(0,s.jsx)(n.li,{children:"-c: number of images to be saved"}),"\n",(0,s.jsx)(n.li,{children:"-w: width of the saved images"}),"\n",(0,s.jsx)(n.li,{children:"-h: height of the saved images"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Result:"}),"\nAfter the program runs correctly, the specified number of image files is saved in the current directory. The ",(0,s.jsx)(n.code,{children:"RAW"})," format is named as ",(0,s.jsx)(n.code,{children:"raw_*.raw"}),", and the ",(0,s.jsx)(n.code,{children:"YUV"})," format is named as ",(0,s.jsx)(n.code,{children:"yuv_*.yuv"}),". The running log is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:/app/cdev_demo/vio_capture$ sudo ./capture -b 12 -c 10 -h 1080 -w 1920\nSetting VPS channel-2: src_w:1920, src_h:1080; dst_w:1920, dst_h:1080;\nSetting VPS channel-1: src_w:1920, src_h:1080; dst_w:1920, dst_h:1080;\njiale:start streaming...\ncapture time :0\ncapture time :1\ncapture time :2\ncapture time :3\ncapture time :4\ncapture time :5\ncapture time :6\ncapture time :7\ncapture time :8\ncapture time :9\nsensor_name imx477, setting_size = 1\n[  701.213210]hb_isp_algo_stop@main_user.c:389 GENERIC(ERR) :g_mutex destroy.\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"camera-image-local-saving-rdk-ultra",children:"Camera Image Local Saving (RDK Ultra)"}),"\n",(0,s.jsxs)(n.p,{children:["This ",(0,s.jsx)(n.code,{children:"vio_capture"})," example demonstrates the image capture of a ",(0,s.jsx)(n.code,{children:"MIPI"})," camera and provides the functionality to save the captured images locally in both ",(0,s.jsx)(n.code,{children:"RAW"})," and ",(0,s.jsx)(n.code,{children:"YUV"})," formats (mutually exclusive). The flowchart of the example is shown below:"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image-capture",src:i(47695).A+"",width:"1280",height:"1055"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Preparation:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["With the development board powered off, connect the ",(0,s.jsx)(n.code,{children:"MIPI"})," camera to the board. Refer to the ",(0,s.jsx)(n.a,{href:"../installation/hardware_interface#mipi_port",children:"MIPI Camera Connection Tutorial"})," for the connection method."]}),"\n",(0,s.jsx)(n.li,{children:"Connect the development board to a display monitor via an HDMI cable."}),"\n",(0,s.jsx)(n.li,{children:"Power on the development board and log in via the command line interface."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"If you need to obtain raw data, please follow the steps below"}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Edit the configuration file of the corresponding camera. Taking ",(0,s.jsx)(n.code,{children:"IMX219"})," as an example, edit ",(0,s.jsx)(n.code,{children:"/etc/camera_configs/Ultra/imx219/1080/vpm.json"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Modify the ",(0,s.jsx)(n.code,{children:"isp_dma_output_format"})," field to ",(0,s.jsx)(n.code,{children:"4"})," and save the changes."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["If you need to obtain images in the ",(0,s.jsx)(n.code,{children:"NV12"})," format, please follow the steps below"]}),":","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Edit the configuration file of the corresponding camera. Taking ",(0,s.jsx)(n.code,{children:"IMX219"})," as an example, edit ",(0,s.jsx)(n.code,{children:"/etc/camera_configs/Ultra/imx219/1080/vpm.json"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Modify the ",(0,s.jsx)(n.code,{children:"isp_stream_output_format"})," field to ",(0,s.jsx)(n.code,{children:"0"}),"; modify the ",(0,s.jsx)(n.code,{children:"isp_dma_output_format"})," field to ",(0,s.jsx)(n.code,{children:"9"}),"; modify the ",(0,s.jsx)(n.code,{children:"pym_mode"})," field to ",(0,s.jsx)(n.code,{children:"0"}),"; and save the changes."]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Execution:"}),"\nThe example code is provided in source code form and needs to be compiled and run using the ",(0,s.jsx)(n.code,{children:"make"})," command. The steps are as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:~$ cd /app/cdev_demo/vio_capture/\nsunrise@ubuntu:/app/cdev_demo/vio_capture$ sudo make\nsunrise@ubuntu:/app/cdev_demo/vio_capture$ sudo ./capture -b 12 -c 10 -h 1080 -w 1920 -m 0\n"})}),"\n",(0,s.jsx)(n.p,{children:"Parameter explanation:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["-b: Bit number of the RAW image, currently set to ",(0,s.jsx)(n.strong,{children:"12"})]}),"\n",(0,s.jsx)(n.li,{children:"-c: Number of images to save"}),"\n",(0,s.jsx)(n.li,{children:"-w: Width of the images to save"}),"\n",(0,s.jsx)(n.li,{children:"-h: Height of the images to save"}),"\n",(0,s.jsx)(n.li,{children:"-m: Type of the images to save, 0: YUV, 1: RAW"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Result:"}),"\nAfter the program runs correctly, the specified number of image files will be saved in the current directory. The ",(0,s.jsx)(n.code,{children:"RAW"})," format is named as ",(0,s.jsx)(n.code,{children:"raw_*.raw"}),", and the ",(0,s.jsx)(n.code,{children:"YUV"})," format is named as ",(0,s.jsx)(n.code,{children:"yuv_*.yuv"}),". The running log is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"root@ubuntu:/app/cdev_demo/media_cdev/vio_capture# sudo ./capture -b 12 -c 10 -h 1080 -w 1920 -m 0\nCamera: gpio_num=432, active=low, i2c_bus=6, mipi_host=3\nCamera: gpio_num=293, active=low, i2c_bus=5, mipi_host=1\nCamera: gpio_num=290, active=low, i2c_bus=4, mipi_host=2\nCamera: gpio_num=289, active=low, i2c_bus=2, mipi_host=0\ncmd=i2ctransfer -y -f 6 w2@0x10 0x0 0x0 r1 2>&1, result=0x02\ncapture time :0\ncapture time :1\ncapture time :2\ncapture time :3 \ncapture time :4 \ncapture time :5 \ncapture time :6 \ncapture time :7 \ncapture time :8 \ncapture time :9 \n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"sensor_name imx219, setting_size = 1"}),"\n",(0,s.jsx)(n.h2,{id:"camera-image-capture-and-encoding",children:"Camera Image Capture and Encoding"}),"\n",(0,s.jsxs)(n.p,{children:["This example ",(0,s.jsx)(n.code,{children:"vio2encoder"})," implements the MIPI camera image capture function and saves the encoded image locally for previewing on a monitor. The flowchart of the example is as follows:\n",(0,s.jsx)(n.img,{alt:"image-vio_to_encoder",src:i(72898).A+"",width:"1280",height:"947"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Preparation:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["With the development board powered off, connect the MIPI camera to the development board. Refer to the ",(0,s.jsx)(n.a,{href:"../installation/hardware_interface#mipi_port",children:"MIPI camera connection tutorial"})," for the connection method."]}),"\n",(0,s.jsx)(n.li,{children:"Connect the development board to the monitor using an HDMI cable."}),"\n",(0,s.jsx)(n.li,{children:"Power on the development board and log in through the command line."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Execution:"})," Execute the program according to the following command.\nThe example code is provided in source code form and needs to be compiled using the ",(0,s.jsx)(n.code,{children:"make"})," command before execution. The steps are as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:~$ cd /app/cdev_demo/vio2encoder\nsunrise@ubuntu:/app/cdev_demo/vio2encoder$ sudo make\nsunrise@ubuntu:/app/cdev_demo/vio2encoder$ sudo ./vio2encoder -w 1920 -h 1080 --iwidth 1920 --iheight 1080 -o test.h264\n"})}),"\n",(0,s.jsx)(n.p,{children:"Parameter explanation:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"-w: Video encoding width"}),"\n",(0,s.jsx)(n.li,{children:"-h: Video encoding height"}),"\n",(0,s.jsx)(n.li,{children:"--iwidth: Sensor output width"}),"\n",(0,s.jsx)(n.li,{children:"--iheight: Sensor output height"}),"\n",(0,s.jsx)(n.li,{children:"-o: Encoding output path"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Result:"}),"\nAfter the program runs successfully, a video file named ",(0,s.jsx)(n.code,{children:"stream.h264"})," will be generated in the current directory. The log during the run is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:/tmp/nfs/sp_cdev/cdev_demo/vio2encoder$ sudo ./vio2encoder -w 1920 -h 1080 --iwidth 1920 --iheight 1080 -o test.h264\n2023/03/28 02:27:32.560 !INFO [x3_cam_init_param][0099]Enable mipi host0 mclk\n2023/03/28 02:27:32.561 !INFO [x3_cam_init_param][0099]Enable mipi host1 mclk\nCamera: gpio_num=114, active=low, i2c_bus=3, mipi_host=0\nCamera: gpio_num=114, active=low, i2c_bus=1, mipi_host=1\nCamera: gpio_num=114, active=low, i2c_bus=0, mipi_host=2\nCamera 0:\n      enable: 1\n      i2c_bus: 3\n      mipi_host: 0\nCamera 1:\n      enable: 1\n      i2c_bus: 1mipi_host: 1\n Camera 2:\n       enable: 1\n       i2c_bus: 0\n       mipi_host: 2\n cmd=i2ctransfer -y -f 3 w2@0x10 0x0 0x0 r1 2>&1, result=0x02\n\n Found sensor:imx219 on i2c bus 3, use mipi host 0\n Setting VPS channel-2: src_w:1920, src_h:1080; dst_w:1920, dst_h:1080;\n Setting VPS channel-1: src_w:1920, src_h:1080; dst_w:1920, dst_h:1080;\n sp_open_camera success!\n sp_start_encode success!\n sp_module_bind(vio -> encoder) success!\n\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"video-file-decoding-and-displaying",children:"Video file decoding and displaying"}),"\n",(0,s.jsxs)(n.p,{children:["This example ",(0,s.jsx)(n.code,{children:"decoder2display"})," implements video file decoding and outputs it through the ",(0,s.jsx)(n.code,{children:"HDMI"})," interface for previewing on a monitor. The flowchart of the example is as follows:",(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.img,{alt:"image-decoder_to_display",src:i(78406).A+"",width:"1280",height:"696"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Preparation\uff1a"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Connect the development board to the monitor using an HDMI cable."}),"\n",(0,s.jsx)(n.li,{children:"Power on the development board and log in via the command line."}),"\n",(0,s.jsxs)(n.li,{children:["Prepare the video file ",(0,s.jsx)(n.code,{children:"stream.h264"})," as the input."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Running\uff1a"}),"\nThe sample code is provided in source code form. You need to use the ",(0,s.jsx)(n.code,{children:"make"})," command to compile and then run. The steps are as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:~$ cd /app/cdev_demo/decode2display\nsunrise@ubuntu:/app/cdev_demo/decode2display$ sudo make\nsunrise@ubuntu:/app/cdev_demo/decode2display$ sudo ./decoder2display -w 1920 -h 1080 -i stream.h264\n"})}),"\n",(0,s.jsx)(n.p,{children:"Parameter description\uff1a"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"-h: height of the video file"}),"\n",(0,s.jsx)(n.li,{children:"-w: width of the video file"}),"\n",(0,s.jsx)(n.li,{children:"-i: the path of the video file"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected result\uff1a"}),"\nAfter the program is running correctly, the video image will be output through the ",(0,s.jsx)(n.code,{children:"HDMI"})," interface of the development board, and the user can preview the video image on the monitor. The running log is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:/app/cdev_demo/decode2display$ sudo ./decoder2display -w 1920 -h 1080 -i stream.h264\ndisp_w=1024, disp_h=600\n[x3_av_open_stream]:[380]:probesize: 5000000\nsp_start_decode success!\nlibiar: hb_disp_set_timing done!\nsp_start_display success!\nsp_open_vps success!\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"rtsp-streaming-decode",children:"RTSP Streaming Decode"}),"\n",(0,s.jsxs)(n.p,{children:["This example ",(0,s.jsx)(n.code,{children:"rtsp2display"})," implements the function of pulling ",(0,s.jsx)(n.code,{children:"rtsp"})," stream, decoding it, and outputting the video image through HDMI, allowing users to preview the image on a monitor. The flowchart of the example is as follows:\n",(0,s.jsx)(n.img,{alt:"rtsp2display",src:i(62687).A+"",width:"1280",height:"702"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Environment Preparation:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Connect the development board to the monitor using an HDMI cable."}),"\n",(0,s.jsx)(n.li,{children:"Power on the development board and login via the command line."}),"\n",(0,s.jsxs)(n.li,{children:["Prepare the ",(0,s.jsx)(n.code,{children:"rtsp"})," stream as the input source."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Running Method:"}),"\nThe example code is provided in source code form and needs to be compiled and run using the ",(0,s.jsx)(n.code,{children:"make"})," command. The steps are as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:~$ cd /app/cdev_demo/rtsp2display\nsunrise@ubuntu:/app/cdev_demo/rtsp2display$ sudo make #There may be some warning messages, which can be ignored.\nsunrise@ubuntu:/app/cdev_demo/decode2display$ sudo ./rtsp2display -i rtsp://admin:admin123@10.96.32.170:554/0 -t tcp\n"})}),"\n",(0,s.jsx)(n.p,{children:"Parameter configuration:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"-i: URL address of the stream"}),"\n",(0,s.jsx)(n.li,{children:"-t: Transport type, TCP/UDP optional"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Result:"}),"\nAfter the program runs correctly, the video image will be output through the ",(0,s.jsx)(n.code,{children:"HDMI"})," interface of the development board, and users can preview the video image on the monitor. The running log is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"sunrise@ubuntu:/app/cdev_demo/rtsp2display$ sudo ./rtsp2display -i rtsp://admin:admin123@10.96.32.170:554/0 -t tcp\navformat_open_input ok!\navformat_find_stream_info ok!\nInput #0, rtsp, from 'rtsp://admin:admin123@10.96.32.170:554/0':\n  Metadata:\n    title           : h264.mp4\nDuration: N/A, start: 0.040000, bitrate: N/A\n  Stream #0:0: Video: h264 (Main), yuvj420p(pc, bt709, progressive), 1920x1080, 25 tbr, 90k tbn, 180k tbc\n  Stream #0:1: Audio: pcm_mulaw, 8000 Hz, 1 channels, s16, 64 kb/s\nInput #1, rtsp, from 'rtsp://admin:admin123@10.96.32.170:554/0':\n  Metadata:\n    title           : h264.mp4\nDuration: N/A, start: 0.040000, bitrate: N/A\n  Stream #1:0: Video: h264 (Main), yuvj420p(pc, bt709, progressive), 1920x1080, 25 tbr, 90k tbn, 180k tbc\n  Stream #1:1: Audio: pcm_mulaw, 8000 Hz, 1 channels, s16, 64 kb/s\nav_dump_format ok!\nrtsp_w:1920,rtsp_h:1080\ndisplay_w:1024,dispaly_h:600\nlibiar: hb_disp_set_timing done!\nsp_open_vps success!\n"})}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Notes:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"When using UDP protocol to transmit the stream, there may be screen flicker due to network packet loss. In this case, switching to TCP protocol can solve the problem."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"vps-scaling-example",children:"VPS Scaling Example"}),"\n",(0,s.jsxs)(n.p,{children:["This example implements video scaling functionality based on the video processing module ",(0,s.jsx)(n.code,{children:"VPS"}),". Users can preview the image through a monitor."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Environment Preparation:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Connect the development board and the monitor via HDMI cable"}),"\n",(0,s.jsx)(n.li,{children:"Power on the development board and log in through the command line"}),"\n",(0,s.jsx)(n.li,{children:"Prepare the image (NV12) and video file (H264) as inputs"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"How to Run:"}),"\nThe sample code is provided in source code format and needs to be compiled and run using the ",(0,s.jsx)(n.code,{children:"make"})," command. The steps are as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:~$ cd /app/cdev_demo/vps\nsunrise@ubuntu:/app/cdev_demo/vps$ sudo make\nsunrise@ubuntu:/app/cdev_demo/vps$ sudo ./vps -m 1 -i stream.h264 -o output.yuv --iheight 1080 --iwidth 1920 --oheight 720 --owidth 1280\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameter Configuration:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"-i: Path of the file to be operated"}),"\n",(0,s.jsx)(n.li,{children:"-iheight: Input height"}),"\n",(0,s.jsx)(n.li,{children:"-iwidth: Input width"}),"\n",(0,s.jsx)(n.li,{children:"-m: Input mode, 1: video stream; 2: NV12 image"}),"\n",(0,s.jsx)(n.li,{children:"-o: Output path"}),"\n",(0,s.jsx)(n.li,{children:"-oheight: Output height"}),"\n",(0,s.jsx)(n.li,{children:"-width: Output width"}),"\n",(0,s.jsx)(n.li,{children:"-skip: (optional) For video stream input, skip the number of frames at the beginning"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Result:"}),"\nAfter the program runs correctly, the processed image file ",(0,s.jsx)(n.code,{children:"output.yuv"})," will be saved in the current directory. The running log is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",children:"sunrise@ubuntu:/app/cdev_demo/vps$ sudo ./vps -m 1 -i stream.h264 -o output.yuv --iheight 1080 --iwidth 1920 --oheight 720 --ow\nidth 1280\n[x3_av_open_stream]:[380]:probesize: 5000000\nhb_vp_deinit success\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"object-detection-algorithm---fcos",children:"Object Detection Algorithm - FCOS"}),"\n",(0,s.jsxs)(n.p,{children:["This example uses the ",(0,s.jsx)(n.code,{children:"FCOS"})," model to implement the object detection algorithm for local video streams. Users can preview the detection results through a monitor."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Environment Preparation:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Connect the development board and the monitor via HDMI cable"}),"\n",(0,s.jsx)(n.li,{children:"Power on the development board and log in through the command line"}),"\n",(0,s.jsx)(n.li,{children:"Prepare the video file (H264) as the input"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"How to Run:"}),"\nThe sample code is provided in source code format and needs to be compiled and run using the ",(0,s.jsx)(n.code,{children:"make"})," command. The steps are as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:~$ cd /app/cdev_demo/bpu/src\nsunrise@ubuntu:/app/cdev_demo/bpu/src$ sudo makesunrise@ubuntu:/app/cdev_demo/bpu/src$ cd bin \nsunrise@ubuntu:/app/cdev_demo/bpu/src/bin$ sudo ./sample -f /app/model/basic/fcos_512x512_nv12.bin -m 1 -i 1080p_.h264 -w 1920 -h 1080\n"})}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameter Configuration:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"-f: Model file path"}),"\n",(0,s.jsx)(n.li,{children:"-h: Height of the input video"}),"\n",(0,s.jsx)(n.li,{children:"-w: Width of the input video"}),"\n",(0,s.jsx)(n.li,{children:"-i: Path of the input video"}),"\n",(0,s.jsx)(n.li,{children:"-m: Model type, default is 1"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output:"}),"\nAfter the program runs successfully, the video and the rendered image after algorithm detection will be output through the ",(0,s.jsx)(n.code,{children:"HDMI"})," interface for user preview on a display. The running log is as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'sunrise@ubuntu:/app/cdev_demo/bpu/src/bin$ sudo ./sample -f /app/model/basic/fcos_512x512_nv12.bin -m 1 -i 1080p_.h264 -w 1920 -h 1080\n[BPU_PLAT]BPU Platform Version(1.3.1)!\n[HBRT] set log level as 0. version = 3.14.5\n[DNN] Runtime version = 1.9.7_(3.14.5 HBRT)\nModel info:\nmodel_name: fcos_512x512_nv12Input count: 1input[0]: tensorLayout: 2 tensorType: 1 validShape:(1, 3, 512, 512, ), alignedShape:(1, 3, 512, 512, )\nOutput count: 15Output[0]: tensorLayout: 0 tensorType: 13 validShape:(1, 64, 64, 80, ), alignedShape:(1, 64, 64, 80, )\nOutput[1]: tensorLayout: 0 tensorType: 13 validShape:(1, 32, 32, 80, ), alignedShape:(1, 32, 32, 80, )\nOutput[2]: tensorLayout: 0 tensorType: 13 validShape:(1, 16, 16, 80, ), alignedShape:(1, 16, 16, 80, )\nOutput[3]: tensorLayout: 0 tensorType: 13 validShape:(1, 8, 8, 80, ), alignedShape:(1, 8, 8, 80, )\nOutput[4]: tensorLayout: 0 tensorType: 13 validShape:(1, 4, 4, 80, ), alignedShape:(1, 4, 4, 80, )\nOutput[5]: tensorLayout: 0 tensorType: 13 validShape:(1, 64, 64, 4, ), alignedShape:(1, 64, 64, 4, )\nOutput[6]: tensorLayout: 0 tensorType: 13 validShape:(1, 32, 32, 4, ), alignedShape:(1, 32, 32, 4, )\nOutput[7]: tensorLayout: 0 tensorType: 13 validShape:(1, 16, 16, 4, ), alignedShape:(1, 16, 16, 4, )\nOutput[8]: tensorLayout: 0 tensorType: 13 validShape:(1, 8, 8, 4, ), alignedShape:(1, 8, 8, 4, )\nOutput[9]: tensorLayout: 0 tensorType: 13 validShape:(1, 4, 4, 4, ), alignedShape:(1, 4, 4, 4, )\nOutput[10]: tensorLayout: 0 tensorType: 13 validShape:(1, 64, 64, 1, ), alignedShape:(1, 64, 64, 1, )\nOutput[11]: tensorLayout: 0 tensorType: 13 validShape:(1, 32, 32, 1, ), alignedShape:(1, 32, 32, 1, )\nOutput[12]: tensorLayout: 0 tensorType: 13 validShape:(1, 16, 16, 1, ), alignedShape:(1, 16, 16, 1, )\nOutput[13]: tensorLayout: 0 tensorType: 13 validShape:(1, 8, 8, 1, ), alignedShape:(1, 8, 8, 1, )\nOutput[14]: tensorLayout: 0 tensorType: 13 validShape:(1, 4, 4, 1, ), alignedShape:(1, 4, 4, 1, )\nlibiar: hb_disp_set_timing done!\ndispaly init ret = 0\nvps open ret = 0\nmodule bind vps & display ret = 0\ndisplay start ret = 0\n[x3_av_open_stream]:[380]:probesize: 5000000\ndecode start ret = 0\nmodule bind decoder & vps ret = 0\n[ERROR]["vdec"][video/src/vdec_group.c:348] [8870.450264]vdec_channel_bump_thread[348]: VDEC_MODULE module try again\n\n[draw_rect]:[137]:========point is 0,return========\nfps:55.555556,processing time:18\n'})}),"\n",(0,s.jsx)(n.h2,{id:"object-detection-algorithm---yolov5",children:"Object Detection Algorithm - YOLOv5"}),"\n",(0,s.jsxs)(n.p,{children:["This example is based on the ",(0,s.jsx)(n.code,{children:"YOLOv5"})," model and implements the camera object detection algorithm, allowing users to preview the detection results on the monitor."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Environment Setup:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["With the development board powered off, connect the ",(0,s.jsx)(n.code,{children:"MIPI"})," camera to the development board following the instructions in the ",(0,s.jsx)(n.a,{href:"../installation/hardware_interface#mipi_port",children:"MIPI Camera Connection Tutorial"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Connect the development board to the monitor using an HDMI cable."}),"\n",(0,s.jsx)(n.li,{children:"Power on the development board and log in via the command line."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Execution:"}),"\nThe sample code is provided in source code form, and it needs to be compiled using the ",(0,s.jsx)(n.code,{children:"make"})," command before execution. The steps are as follows:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:~$ cd /app/cdev_demo/bpu/src\nsunrise@ubuntu:/app/cdev_demo/bpu/src$ sudo make\nsunrise@ubuntu:/app/cdev_demo/bpu/src$ cd bin \nsunrise@ubuntu:/app/cdev_demo/bpu/src/bin$ sudo ./sample -f /app/model/basic/yolov5_672x672_nv12.bin -m 0\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameter Configuration:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"-f: path to the model"}),"\n",(0,s.jsx)(n.li,{children:"-m: model type, default is 0"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Result:"}),"\nAfter the program runs correctly, the video output via the ",(0,s.jsx)(n.code,{children:"HDMI"})," interface will display the rendered image with the algorithm detection. Users can preview it on the monitor. The following is the log during execution:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sunrise@ubuntu:/app/cdev_demo/bpu/src/bin$ sudo ./sample -f /app/model/basic/yolov5_672x672_nv12.bin -m 0\n[BPU_PLAT]BPU Platform Version(1.3.1)!\n[HBRT] set log level as 0. version = 3.14.5\n[DNN] Runtime version = 1.9.7_(3.14.5 HBRT)\nModel info:\nmodel_name: yolov5_672x672_nv12Input count: 1input[0]: tensorLayout: 2 tensorType: 1 validShape:(1, 3, 672, 672, ), alignedShape:(1, 3, 672, 672, )\nOutput count: 3Output[0]: tensorLayout: 0 tensorType: 13 validShape:(1, 84, 84, 255, ), alignedShape:(1, 84, 84, 255, )\nOutput[1]: tensorLayout: 0 tensorType: 13 validShape:(1, 42, 42, 255, ), alignedShape:(1, 42, 42, 255, )\nOutput[2]: tensorLayout: 0 tensorType: 13 validShape:(1, 21, 21, 255, ), alignedShape:(1, 21, 21, 255, )\nSetting VPS channel-1: src_w:1920, src_h:1080; dst_w:672, dst_h:672;\nSetting VPS channel-3: src_w:1920, src_h:1080; dst_w:1024, dst_h:600;\nSetting VPS channel-2: src_w:1920, src_h:1080; dst_w:1920, dst_h:1080;\nstart linear mode, sensor_name f37, setting_size = 3\nlibiar: hb_disp_set_timing done!\nyolov5_do_post fps:11.627907,processing time :86\n"})}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},47695:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/image-capture-93adaad7590d2b55576023a1b3facc55.jpg"},78406:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/image-decoder_to_display-1e28f7436f414b7547db2617e4cf38ea.jpg"},62687:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/image-rtsp_to_display-61e297d388aef02fc815163312b9105b.jpg"},96600:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/image-vio_to_display-c872ce929f87ba158322c3335de8e002.jpg"},72898:(e,n,i)=>{i.d(n,{A:()=>s});const s=i.p+"assets/images/image-vio_to_encoder-d2ef3306047b1fe0c94b6ad43c3e0503.jpg"},28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var s=i(96540);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);