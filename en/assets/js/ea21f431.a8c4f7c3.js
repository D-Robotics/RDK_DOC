"use strict";(self.webpackChunkrdk_doc=self.webpackChunkrdk_doc||[]).push([[9164],{29306:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>t,metadata:()=>a,toc:()=>d});var s=r(74848),o=r(28453);const t={sidebar_position:5},i="8.5 Algorithm toolchain class",a={id:"FAQ/toolchain",title:"8.5 Algorithm toolchain class",description:"Model quantization errors and solutions",source:"@site/i18n/en/docusaurus-plugin-content-docs/current/08_FAQ/05_toolchain.md",sourceDirName:"08_FAQ",slug:"/FAQ/toolchain",permalink:"/rdk_doc/en/FAQ/toolchain",draft:!1,unlisted:!1,editUrl:"https://d-robotics.github.io/rdk_doc/RDK/docs/08_FAQ/05_toolchain.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"8.4 Multimedia Class",permalink:"/rdk_doc/en/FAQ/multimedia"},next:{title:"9. \u9644\u5f55",permalink:"/rdk_doc/en/Appendix"}},l={},d=[{value:"Model quantization errors and solutions <code>\\</code>",id:"model_convert_errors_and_solutions",level:3},{value:"hb_mapper checker (01_check.sh) Model Validation Error",id:"hb_mapper-checker-01_checksh-model-validation-error",level:4},{value:"Error in hb_mapper makertbin (03_build.sh) Model Conversion",id:"error-in-hb_mapper-makertbin-03_buildsh-model-conversion",level:4},{value:"Algorithm Model Boarding Errors and Solutions",id:"algorithm-model-boarding-errors-and-solutions",level:3},{value:"Model Quantization and Board Usage Tips",id:"model-quantization-and-board-usage-tips",level:3},{value:"Transformer Usage Guide",id:"transformer-usage-guide",level:4},{value:"Model Accuracy Optimization Checklist",id:"checklist",level:4},{value:"1. Validate the inference results of the float ONNX model",id:"1-validate-the-inference-results-of-the-float-onnx-model",level:5},{value:"2. Verify the correctness of the YAML configuration file and preprocessing/postprocessing code",id:"2-verify-the-correctness-of-the-yaml-configuration-file-and-preprocessingpostprocessing-code",level:5},{value:"3. Validate that no accuracy loss was introduced during graph optimization",id:"3-validate-that-no-accuracy-loss-was-introduced-during-graph-optimization",level:5},{value:"4. Verify the quantization accuracy meets expectations",id:"4-verify-the-quantization-accuracy-meets-expectations",level:5},{value:"Quantization YAML Configuration File Templates",id:"quantization-yaml-configuration-file-templates",level:4},{value:"RDK X3 Caffe Model Quantization YAML Template",id:"rdk_x3_caffe_yaml_template",level:5},{value:"RDK X3 ONNX Model Quantization YAML Template",id:"rdk_x3_onnx_yaml_template",level:5}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h3:"h3",h4:"h4",h5:"h5",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"85-algorithm-toolchain-class",children:"8.5 Algorithm toolchain class"}),"\n",(0,s.jsxs)(n.h3,{id:"model_convert_errors_and_solutions",children:["Model quantization errors and solutions ",(0,s.jsx)(n.code,{children:"\\"})]}),"\n",(0,s.jsx)(n.h4,{id:"hb_mapper-checker-01_checksh-model-validation-error",children:"hb_mapper checker (01_check.sh) Model Validation Error"}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Issue\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR The shape of model input:input is [xxx] which has dimensions of 0. Please specify input-shape parameter.\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This error occurs when the model input has a dynamic shape. To resolve it, you can specify the input shape using the ",(0,s.jsx)(n.code,{children:"--input-shape input_name input_shape"})," parameter."]}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Issue\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR HorizonRT does not support these CPU operators: {op_type}\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error happens when the CPU operator being used is unsupported by HorizonRT. To address this, check the list of supported operators provided and replace the unsupported one. If the unsupported operator is crucial to your model, contact HorizonRT for development evaluation."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Issue\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Unsupported op {op_type}\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error results from using an unsupported BPU operator in your model. If the model's overall performance meets your requirements, you can ignore this log. However, if performance expectations are not met, consider replacing the unsupported operator with a supported one from the provided operator list."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Issue\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR nodes:['{op_type}'] are specified as domain:xxx, which are not supported by official ONNX. Please check whether these ops are official ONNX ops or defined by yourself \n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The reason for this error could be that the custom operator used is not supported by D-Robotics. To resolve it, you can either replace the operator with one listed in our supported operator list or develop and register a custom CPU operator."}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"error-in-hb_mapper-makertbin-03_buildsh-model-conversion",children:"Error in hb_mapper makertbin (03_build.sh) Model Conversion"}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Layer {op_name}  \n    expects data shape within [[xxx][xxx]], but received [xxx]\nLayer {op_name}\n    Expected tensor xxx to have n dimensions, but found m\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This error might occur if the ",(0,s.jsx)(n.code,{children:"{op_name}"})," operator is falling back to CPU computation due to unsupported dimensions. If the performance loss from using CPU is acceptable, you can ignore this message. However, if performance is a concern, review the operator support list and modify the op to a BPU-supported configuration."]}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"INFO: Layer {op_name} will be executed on CPU\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This error indicates that ",(0,s.jsx)(n.code,{children:"{op_name}"})," operator is being computed on CPU because its shape (CxHxW) exceeds the limit of 8192. If only a few operators are affected and the overall model performance meets expectations, there's no need to worry. However, if performance is unsatisfactory, consider examining the operator support list for alternatives without shape limitations on BPU."]}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR There is an error in pass: {op_name}. Error message:xxx\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This error might stem from an optimization failure of the ",(0,s.jsx)(n.code,{children:"{op_name}"})," operator. To address this issue, please gather your model and .log files and provide them to D-Robotics technical support for analysis and resolution."]}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Error There is an error in pass:constant_folding. Error message: Could not find an implementation for the node {op_name}\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.p,{children:["This error typically occurs when ONNX Runtime encounters an operator (",(0,s.jsx)(n.code,{children:"op_name"}),") that it does not have a built-in implementation for. It might be a custom or unsupported operator in the current version of ORT. To resolve this issue, you should verify if the specific operator is supported by checking the ORT operator list. If it's a core operator, consider contacting D-Robotics for development assessment or seeking an alternative implementation."]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"WARNING input shape [xxx] has length: n  ERROR list index out of range\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.p,{children:["This warning indicates that the input shape provided to the model is not compatible with its requirements, as it expects a four-dimensional shape (e.g., HxW for a 2D image), but received a shape with length ",(0,s.jsx)(n.code,{children:"n"})," which is not recognized as a standard format. To fix this, ensure your input data is reshaped into a 4D tensor (e.g., change ",(0,s.jsx)(n.code,{children:"xxx"})," to ",(0,s.jsx)(n.code,{children:"1x1xHxW"}),")."]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Start to parse the onnx model\ncore dump\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsx)(n.p,{children:"This error suggests that there was a failure during the parsing of the ONNX model. It could be due to missing or invalid information, such as naming issues with output or input nodes. Ensure that the exported ONNX model is properly formatted and that all necessary nodes have unique names. If the problem persists, check the model file for any syntax errors and consult the ONNX documentation."}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"Start to calibrate/quantize the model\ncore dump\n\nStart to compile the model \ncore dump\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error may occur because of a model quantization/compilation failure. In response to this error, please collect the model and.log file and provide it to D-Robotics technicians for analysis."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR model conversion faild: Inferred shape and existing shape differ in dimension x: (n) vs (m)\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error may occur because the input shape of the onnx model is illegal, or because the tool optimization pass is incorrect. In response to this error, please ensure that the onnx model is valid, and if the onnx model can be reasoned, please provide the model to D-Robotics technicians for analysis and processing."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"WARNING got unexpected input/output/sumin threshold on conv {op_name}! value: xxx\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error may occur because of incorrect data preprocessing, or because the weight value of the node is too small/too large. In response to this error, 1. Please check whether the data preprocessing is incorrect; 2. We recommend that you use BN operator to optimize data distribution."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR hbdk-cc compile hbir model failed with returncode -n\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error may occur because the model failed to compile. In response to this error, please collect the model and.log file and provide it to D-Robotics technicians for analysis."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR {op_type}  only support 4 dim input\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error may occur because the toolchain does not yet support the op input dimension as non-four-dimensional. In response to this error, we recommend that you adjust the op input dimension to four dimensions."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR {op_type} Not support this attribute/mode=xxx\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error may occur because the tool chain does not yet support this property of the op. For this error, you can replace it based on the operator support list we provide or contact D-Robotics for a development evaluation."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR There is no node can execute on BPU in this model, please make sure the model has at least one conv node which is supported by BPU.\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error may occur because there are no quantifiable BPU nodes in the model. In response to this error, ensure that the onnx model is valid and that at least one conv is used in the model. If the preceding conditions are met, collect the model and.log files and provide them to D-Robotics technicians for analysis and processing."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : unable to find an implementation for the node with name: {op_name}, type: {op_type}, opset version.\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This error arises when the model's opset version exceeds what the toolchain supports. To resolve this, re-export the model, ensuring that ",(0,s.jsx)(n.code,{children:"opset_version"})," is set to ",(0,s.jsx)(n.code,{children:"10"})," or ",(0,s.jsx)(n.code,{children:"11"}),"."]}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  ERROR The opset version of the onnx model is n, only model with opset_version 10/11 is supported \n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This error occurs due to an unsupported opset version in the model. To rectify the issue, re-export the model with a compatible version, specifically setting ",(0,s.jsx)(n.code,{children:"opset_version"})," to ",(0,s.jsx)(n.code,{children:"10"})," or ",(0,s.jsx)(n.code,{children:"11"}),"."]}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"After using run_on_bpu, the conversion fails.\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This error might occur due to the unsupported usage of the ",(0,s.jsx)(n.code,{children:"run_on_bpu"})," operator at the moment. ",(0,s.jsx)(n.code,{children:"run_on_bpu"})," currently only supports operators like ",(0,s.jsx)(n.code,{children:"Relu"}),", ",(0,s.jsx)(n.code,{children:"Softmax"}),", and pooling (e.g., ",(0,s.jsx)(n.code,{children:"maxpool"}),", ",(0,s.jsx)(n.code,{children:"avgpool"}),") at the end of the model, as well as CPU*+Transpose combinations (where you can specify a Transpose node name to run CPU* operations on BPU). If your model meets these conditions but still encounters issues, please contact D-Robotics technical support for further analysis. If it doesn't meet the criteria, you can request a development evaluation."]}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR: tool limits for max output num is 32\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The error is likely due to a limitation in the toolchain that allows a maximum of 32 model output nodes. To resolve this, ensure your model has no more than 32 output nodes."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR: xxx file parse failed.\nERROR: xxx does not exist in xxx.\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error may stem from incorrect environment setup. Please use the provided Docker environment for quantization."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR: exception in command: makertbin.\nERROR: cannot reshape array of size xxx into shape xxx.\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The error is likely related to a preprocessing issue. Please refer to our documentation for relevant information on data preprocessing."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR: load cal data for input xxx error\nERROR: cannot reshape array of size xxx into shape xxx\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error could be due to an incompatible toolchain version. Ensure you are using the corresponding SDK toolchain version provided."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR [ONNXRuntimeError] : 1 : FAIL : Non-zero status code returned while running HzCalibration node.Name:'xxx'Status Message :CUDA error cudaErrorNoKernelImageForDevice:no kernel image is available for execution on the device\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The error might indicate a Docker loading issue. Try using the nvidia-docker loading command when loading Docker."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'[ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from xxx.onnx failed:This is an invalid model. In Node, ("xxx", HzSQuantizedPreprocess, "", -1) : ("images": tensor(int8),"xxx": tensor(int8),"xxx": tensor(int32),"xxx": tensor(int8),) -> ("xxx": tensor(int8),) , Error No Op registered for HzSQuantizedPreprocess with domain_version of 11\n'})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error suggests a mismatch between ONNX versions. Re-export your ONNX model with the opset version 10 and use OpenCV for preprocessing."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"[E:onnxruntime:, sequential_executor.cc:183 Execute] Non-zero status code returned while running Resize node. Name:'xxx' Status Message: upsample.h:299 void onnxruntime::UpsampleBase::ScalesValidation(const std::vector<float>&, onnxruntime::UpsampleMode) const scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1) was false. 'Linear' mode and 'Cubic' mode only support 2-D inputs ('Bilinear', 'Bicubic') or 4-D inputs with the corresponding outermost 2 scale values being 1 in the Resize operator\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"The error is possibly related to ONNXRuntime's internal logic. Since the model contains reshape operations, batch calibration is not possible, and it can only handle images individually. This should not affect the final results."}),"\n"]}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Question\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ERROR: No guantifiable nodes were found, and the model is not supported\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Answer\u3011"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"This error occurs when no quantifiable nodes are found in the model structure, indicating that the model is not compatible for quantization."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"algorithm-model-boarding-errors-and-solutions",children:"Algorithm Model Boarding Errors and Solutions"}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Problem\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"(common.h:79): HR:ERROR: op_name:xxx invalid attr key xxx\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Solution\u3011"}),"\n",(0,s.jsx)(n.p,{children:"This error might occur because the specified attribute key for the op is not supported by libDNN. To resolve it, you can either replace the unsupported op with a compatible one from our operator support list or contact D-Robotics for further development evaluation."}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Problem\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"(hb_dnn_ndarray.cpp:xxx): data type of ndarray do not match specified type. NDArray dtype_: n, given\uff1am\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Solution\u3011"}),"\n",(0,s.jsx)(n.p,{children:"The error arises when the input data type does not match the required type for the operator. libDNN currently lacks support for this input type; we will gradually enforce operator constraints during the model conversion phase. To fix, check our operator support list and consider replacing or contacting D-Robotics for development assessment."}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Problem\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"(validate_util.cpp:xxx): tensor aligned shape size is xxx , but tensor hbSysMem memSize is xxx, tensor hbSysMem memSize should >= tensor aligned shape size!\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Solution\u3011"}),"\n",(0,s.jsx)(n.p,{children:"This error happens when the allocated memory for the input data is insufficient. To address this, ensure that you allocate memory usinghbDNNTensorProperties.alignedByteSize function to accommodate the required size."}),"\n",(0,s.jsx)("font",{color:"Blue",children:"\u3010Problem\u3011"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"(bpu_model_info.cpp:xxx): HR:ERROR: hbm model input feature names must be equal to graph node input names\n"})}),"\n",(0,s.jsx)("font",{color:"Green",children:"\u3010Solution\u3011"}),"\n",(0,s.jsx)(n.p,{children:"An error occurred because the input feature names for the HBM model must match the node input names in the graph. Make sure to verify and align the input names accordingly."}),"\n",(0,s.jsx)(n.h3,{id:"model-quantization-and-board-usage-tips",children:"Model Quantization and Board Usage Tips"}),"\n",(0,s.jsx)(n.h4,{id:"transformer-usage-guide",children:"Transformer Usage Guide"}),"\n",(0,s.jsx)(n.p,{children:"This section will provide explanations of various transformers and their parameters, along with usage examples to assist you in working with transformers."}),"\n",(0,s.jsx)(n.p,{children:"Before diving into the content, please note:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The image data is in ",(0,s.jsx)(n.strong,{children:"three-dimensional format"}),"; however, D-Robotics's transformers operate on ",(0,s.jsx)(n.strong,{children:"four-dimensional"})," data. Transformers only apply operations to the ",(0,s.jsx)(n.strong,{children:"first channel"})," of input images."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"AddTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nPerforms an addition operation on all pixel values in the input image. This transformer converts the output data format to float32."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"value: The value to add to each pixel, which can be negative, like -128."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Subtract 128 from image data\n  AddTransformer(-128)\n\n  # Add 127 to image data\n  AddTransformer(127)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"MeanTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nSubtracts the mean_value from all pixel values in the input image."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"means: The value to subtract from each pixel, which can be negative, like -128."}),"\n",(0,s.jsx)(n.li,{children:'data_format: Input layout type, can be either "CHW" or "HWC", default is "CHW".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'  # Subtract 128.0 from each pixel (CHW format)\n  MeanTransformer(np.array([128.0, 128.0, 128.0]))\n\n  # Subtract different values for each channel (HWC format)\n  MeanTransformer(np.array([103.94, 116.78, 123.68]), data_format="HWC")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ScaleTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nMultiplies all pixel values in the input image by the scale_value."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"scale_value: The coefficient to multiply by, such as 0.0078125 or 1/128."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Adjust pixel range from -128 to 127 to -1 to 1\n  ScaleTransformer(0.0078125)\n  # Or\n  ScaleTransformer(1/128)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NormalizeTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nNormalizes the input image by performing a scaling operation. The transformer converts the output data format to float32."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"std: The value to divide each pixel by, typically the standard deviation of the first image."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Normalize pixel range [-128, 127] to -1 to 1\n  NormalizeTransformer(128)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"TransposeTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nPerforms a layout transformation on the input image."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"order: The new order of dimensions after the transformation (related to the original layout). For example, for HWC, the order would be (2, 0, 1) to convert to CHW."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Convert HWC to CHW\n  TransposeTransformer((2, 0, 1))\n  # Convert CHW to HWC\n  TransposeTransformer((1, 2, 0))\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"HWC2CHWTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nTransforms input from NHWC to NCHW layout."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"None."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Convert NHWC to NCHW\n  HWC2CHWTransformer()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"CHW2HWCTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nTransforms input from NCHW to NHWC layout."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"None."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Convert NCHW to NHWC\n  CHW2HWCTransformer()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"CenterCropTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nCrops a square image from the center of the input image. The transformer converts the output data format to float32, and uint8 if ",(0,s.jsx)(n.code,{children:"data_type"})," is set to uint8."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"crop_size: The side length of the square to crop."}),"\n",(0,s.jsx)(n.li,{children:'data_type: Output data type, can be "float" or "uint8".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'  # Crop a 224x224 center, default float32 output\n  CenterCropTransformer(crop_size=224)\n\n  # Crop a 224x224 center, output as uint8\n  CenterCropTransformer(crop_size=224, data_type="uint8")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"PILCenterCropTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nUses PIL to crop a square image from the center of the input."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"size: The side length of the square to crop."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Crop a 224x224 center using PIL\n  PILCenterCropTransformer(size=224)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"LongSideCropTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nCrops the longest edge of the input image while maintaining aspect ratio. The transformer converts the output data format to float32."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"None."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  LongSideCropTransformer()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"PadResizeTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nEnlarges the image by padding and resizing. The transformer converts the output data format to float32."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"target_size: Target size as a tuple, e.g., (240, 240)."}),"\n",(0,s.jsx)(n.li,{children:"pad_value: Value to pad the array with, default is 127."}),"\n",(0,s.jsx)(n.li,{children:'pad_position: Padding position, can be "boundary" or "bottom_right", default is "boundary".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Resize to 512x512, pad to bottom-right corner, pad value is 0\n  PadResizeTransformer((512, 512), pad_position='bottom_right', pad_value=0)\n\n  # Resize to 608x608, pad to edges, pad value is 127\n  PadResizeTransformer(target_size=(608, 608))\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ResizeTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nResizes the image to the specified target size."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"target_size: Target size as a tuple, e.g., (240, 240)."}),"\n",(0,s.jsx)(n.li,{children:'mode: Image processing mode, can be "skimage" or "opencv", default is "skimage".'}),"\n",(0,s.jsx)(n.li,{children:'method: Interpolation method, only used when mode is "skimage". Range is 0-5, default is 1 (bicubic).'}),"\n",(0,s.jsx)(n.li,{children:"data_type: Output data type, can be uint8 or float, default is float."}),"\n",(0,s.jsx)(n.li,{children:'interpolation: Interpolation method, only used when mode is "opencv". Can be empty (default INTER_LINEAR) or one of OpenCV\'s interpolation methods.'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Resize to 224x224, using opencv with bilinear interpolation, float32 output\n  ResizeTransformer(target_size=(224, 224), mode='opencv', method=1)\n\n  # Resize to 256x256, using skimage with bilinear interpolation, float32 output\n  ResizeTransformer(target_size=(256, 256))\n\n  # Resize to 256x256, using skimage with bilinear interpolation, uint8 output\n  ResizeTransformer(target_size=(256, 256), data_type=\"uint8\")\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"PILResizeTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":\nResizes the image using the PIL library."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"size: Target size as a tuple, e.g., (240, 240)."}),"\n",(0,s.jsx)(n.li,{children:"interpolation: PIL interpolation method, options include Image.NEAREST, Image.BILINEAR, Image.BICUBIC, Image.LANCZOS, default is Image.BILINEAR."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"  # Resize the input image to the specified size using PIL\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"LinearResizeTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Resize the input image to a size of 256x256 using linear interpolation."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"PILResizeTransformer(size=256)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ShortLongResizeTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Performs resizing based on the original aspect ratio, with output dimensions determined by provided parameters. The process involves:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Scaling by a factor calculated as ",(0,s.jsx)(n.code,{children:"short_size"})," divided by the smaller dimension of the original image."]}),"\n",(0,s.jsxs)(n.li,{children:["If the scaled maximum dimension exceeds ",(0,s.jsx)(n.code,{children:"long_size"}),", the scaling factor adjusts to ",(0,s.jsx)(n.code,{children:"long_size"})," divided by the larger original dimension."]}),"\n",(0,s.jsxs)(n.li,{children:["Uses OpenCV's ",(0,s.jsx)(n.code,{children:"resize"})," method with the calculated scale factor to crop the image."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"short_size: Target length for the shorter side."}),"\n",(0,s.jsx)(n.li,{children:"long_size: Target length for the longer side."}),"\n",(0,s.jsx)(n.li,{children:"include_im: Default True, if True, returns both processed image and the original."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ShortLongResizeTransformer(short_size=20, long_size=100)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"PadTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Resize the image by padding to the desired target size using a scaling factor based on the original dimensions and a divisor."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"size_divisor: Division factor, default 128."}),"\n",(0,s.jsx)(n.li,{children:"target_size: Desired target size, default 512."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"PadTransformer(size_divisor=1024, target_size=1024)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ShortSideResizeTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Explanation"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Crops the image to the specified short side size while maintaining the aspect ratio, using either float32 or uint8 output type and a specified interpolation method."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"short_size: Expected length of the shorter side."}),"\n",(0,s.jsx)(n.li,{children:'data_type: Output data type, can be "float" or "uint8", default "float32".'}),"\n",(0,s.jsx)(n.li,{children:"interpolation: Interpolation method, accepts OpenCV interpolation types, defaults to None (uses INTER_LINEAR)."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Examples"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Resize to 256 with bilinear interpolation\nShortSideResizeTransformer(short_size=256)\n\n# Resize to 256 with Lanczos4 interpolation\nShortSideResizeTransformer(short_size=256, interpolation=Image.LANCZOS)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"PaddedCenterCropTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Performs center cropping with padding, specifically designed for EfficientNet-lite models."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Works only for EfficientNet-lite instances."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Calculation Process"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Calculate the coefficient as ",(0,s.jsx)(n.code,{children:"int((float(image_size) / (image_size + crop_pad)))"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Compute the center size as the coefficient times the smaller of the original height and width."}),"\n",(0,s.jsx)(n.li,{children:"Crop the image centered around the calculated size."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"image_size: Image size, default 224."}),"\n",(0,s.jsx)(n.li,{children:"crop_pad: Padding amount, default 32."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"PaddedCenterCropTransformer(image_size=240, crop_pad=32)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"BGR2RGBTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Converts the input format from BGR to RGB."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameter"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Data layout, can be "CHW" or "HWC", default "CHW".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# For NCHW layout, convert BGR to RGB\nBGR2RGBTransformer()\n\n# For NHWC layout, convert BGR to RGB\nBGR2RGBTransformer(data_format="HWC")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"RGB2BGRTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Converts the input format from RGB to BGR."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameter"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Data layout, can be "CHW" or "HWC", default "CHW".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# For NCHW layout, convert RGB to BGR\nRGB2BGRTransformer()\n\n# For NHWC layout, convert RGB to BGR\nRGB2BGRTransformer(data_format="HWC")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"RGB2GRAYTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Converts the input format from RGB to grayscale."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameter"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Input layout, can be "CHW" or "HWC", default "CHW".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# For NCHW layout, convert RGB to grayscale\nRGB2GRAYTransformer(data_format='CHW')\n\n# For NHWC layout, convert RGB to grayscale\nRGB2GRAYTransformer(data_format='HWC')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"BGR2GRAYTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Converts the input format from BGR to grayscale."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameter"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Input layout, can be "CHW" or "HWC", default "CHW".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# For NCHW layout, convert BGR to grayscale\nBGR2GRAYTransformer(data_format='CHW')\n\n# For NHWC layout, convert BGR to grayscale\nBGR2GRAYTransformer(data_format='HWC')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"RGB2GRAY_128Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Converts the input format from RGB to grayscale with values ranging from -128 to 127."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameter"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Input layout, must be "CHW" or "HWC".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# For NCHW layout, convert RGB to 128-bit grayscale\nRGB2GRAY_128Transformer(data_format='CHW')\n\n# For NHWC layout, convert RGB to 128-bit grayscale\nRGB2GRAY_128Transformer(data_format='HWC')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"RGB2YUV444Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Converts the input format from RGB to YUV444."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameter"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Input layout, can be "CHW" or "HWC", required.'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# For NCHW layout, convert RGB to YUV444\nBGR2YUV444Transformer(data_format='CHW')\n\n# For NHWC layout, convert RGB to YUV444\nBGR2YUV444Transformer(data_format='HWC')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"BGR2YUV444Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Converts the input format from BGR to YUV444."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameter"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Input layout, can be "CHW" or "HWC", required.'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# For NCHW layout, convert BGR to YUV444\nBGR2YUV444Transformer(data_format='CHW')\n\n# For NHWC layout, convert BGR to YUV444\nBGR2YUV444Transformer(data_format='HWC')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"BGR2YUV444_128Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Converts the input format from BGR to YUV444 with values ranging from -128 to 127."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameter"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Input layout, must be "CHW" or "HWC".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# For NCHW layout, convert BGR to 128-bit YUV444\nBGR2YUV444_128Transformer(data_format='CHW')\n\n# For NHWC layout, convert BGR to 128-bit YUV444\nBGR2YUV444_128Transformer(data_format='HWC')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"RGB2YUV444_128Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":"]}),"\n",(0,s.jsx)(n.p,{children:"Converts the input format from RGB to YUV444 with values ranging from -128 to 127."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameter"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Input layout, can be "CHW" or "HWC".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# For NCHW layout, convert RGB to 128-bit YUV444\nRGB2YUV444_128Transformer(data_format='CHW')\n\n# For NHWC layout, convert RGB to 128-bit YUV444\nRGB2YUV444_128Transformer(data_format='HWC')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"BGR2YUVBT601VIDEOTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nTransforms the input format from BGR to YUV_BT601_Video_Range."]}),"\n",(0,s.jsx)(n.p,{children:"YUV_BT601_Video_Range: Some camera inputs are in YUV BT601 (Video Range) format with values ranging from 16 to 235. This transformer is designed for such data."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: The input layout type, can be either "CHW" or "HWC", default is "CHW". This is a required field.'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# For NCHW layout, convert BGR to YUV_BT601_Video_Range\nBGR2YUVBT601VIDEOTransformer(data_format='CHW')\n\n# For NHWC layout, convert BGR to YUV_BT601_Video_Range\nBGR2YUVBT601VIDEOTransformer(data_format='HWC')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"RGB2YUVBT601VIDEOTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nSimilar to BGR2YUVBT601VIDEOTransformer but for RGB input."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: The input layout type, can be either "CHW" or "HWC", default is "CHW". This is a required field.'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# For NCHW layout, convert RGB to YUV_BT601_Video_Range\nRGB2YUVBT601VIDEOTransformer(data_format='CHW')\n\n# For NHWC layout, convert RGB to YUV_BT601_Video_Range\nRGB2YUVBT601VIDEOTransformer(data_format='HWC')\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"YUVTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nConverts the input format to YUV444."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"color_sequence: The color sequence, a required field."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Convert an image read as BGR to YUV444\nYUVTransformer(color_sequence="BGR")\n\n# Convert an image read as RGB to YUV444\nYUVTransformer(color_sequence="RGB")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ReduceChannelTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nReduces the C channel to a single channel. This transformer is mainly for handling channels like converting a shape of 1",(0,s.jsx)(n.em,{children:"3"}),"224",(0,s.jsx)(n.em,{children:"224 to 1"}),"1",(0,s.jsx)(n.em,{children:"224"}),"224. Ensure the layout matches the data_format to avoid removing the wrong channel."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: The input layout type, can be either "CHW" or "HWC", default is "CHW".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Remove the C channel for NCHW layout\nReduceChannelTransformer()\n\n# Remove the C channel for NHWC layout\nReduceChannelTransformer(data_format="HWC")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"BGR2NV12Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nTranslates input format from BGR to NV12."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Input layout type, can be "CHW" or "HWC", default is "CHW".'}),"\n",(0,s.jsxs)(n.li,{children:['cvt_mode: Conversion mode, can be "rgb_calc" or "opencv", default is "rgb_calc".',"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"rgb_calc: Merges UV using a custom method."}),"\n",(0,s.jsx)(n.li,{children:"opencv: Uses OpenCV's method."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# For NCHW layout, convert BGR to NV12 with rgb_calc mode\nBGR2NV12Transformer()\n\n# For NHWC layout, convert BGR to NV12 with opencv mode\nBGR2NV12Transformer(data_format="HWC", cvt_mode="opencv")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"RGB2NV12Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nSimilar to BGR2NV12Transformer but for RGB input."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'data_format: Input layout type, can be "CHW" or "HWC", default is "CHW".'}),"\n",(0,s.jsx)(n.li,{children:'cvt_mode: Conversion mode, can be "rgb_calc" or "opencv", default is "rgb_calc".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# For NCHW layout, convert RGB to NV12 with rgb_calc mode\nRGB2NV12Transformer()\n\n# For NHWC layout, convert RGB to NV12 with opencv mode\nRGB2NV12Transformer(data_format="HWC", cvt_mode="opencv")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NV12ToYUV444Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nTransforms input format from NV12 to YUV444."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"target_size: The desired size as a tuple, e.g., (240, 240)."}),"\n",(0,s.jsx)(n.li,{children:'yuv444_output_layout: The layout for the YUV444 output, can be "HWC" or "CHW", default is "HWC".'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# For NCHW layout and input size of 768*768, convert NV12 to YUV444\nNV12ToYUV444Transformer(target_size=(768, 768))\n\n# For NHWC layout and input size of 224*224, convert NV12 to YUV444\nNV12ToYUV444Transformer((224, 224), yuv444_output_layout="HWC")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"WarpAffineTransformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nPerforms image affine transformation."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"input_shape: The input shape value."}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"scale: The scaling factor."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# For an image of size 512*512, scale the longer side by 1.0\nWarpAffineTransformer((512, 512), 1.0)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"F32ToS8Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nConverts input format from float32 to int8."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":\nNo parameters."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Convert input from float32 to int8\nF32ToS8Transformer()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"F32ToU8Transformer"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Description"}),":\nConverts input format from float32 to uint8."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":\nNo parameters."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Convert input from float32 to uint8\nF32ToU8Transformer()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example usage guide for YOLOv5x model"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"YOLOv5x Model:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["Download the corresponding .pt file from the URL: ",(0,s.jsx)(n.a,{href:"https://github.com/ultralytics/yolov5/releases/tag/v2.0",children:"yolov5-2.0"}),". Make sure you use the tag ",(0,s.jsx)(n.code,{children:"v2.0"})," to ensure successful conversion."]}),"\n",(0,s.jsx)(n.p,{children:"MD5SUMs:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"MD5SUM"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"File"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"2e296b5e31bf1e1b6b8ea4bf36153ea5"}),(0,s.jsx)(n.td,{children:"yolov5l.pt"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"16150e35f707a2f07e7528b89c032308"}),(0,s.jsx)(n.td,{children:"yolov5m.pt"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"42c681cf466c549ff5ecfe86bcc491a0"}),(0,s.jsx)(n.td,{children:"yolov5s.pt"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"069a6baa2a741dec8a2d44a9083b6d6e"}),(0,s.jsx)(n.td,{children:"yolov5x.pt"})]})]})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:"Modify the YOLOv5 code from GitHub (version v2.0) for better compatibility with post-processing:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["In models/yolo.py, remove the reshape and layout change at the end of each output branch.(",(0,s.jsx)(n.a,{href:"https://github.com/ultralytics/yolov5/blob/v2.0/models/yolo.py",children:"https://github.com/ultralytics/yolov5/blob/v2.0/models/yolo.py"}),")"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"\n    def forward(self, x):\n        # x = x.copy()  # for profiling\n        z = []  # inference output\n        self.training |= self.export\n        for i in range(self.nl):\n            x[i] = self.m[i](x[i])  # conv\n            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n            #  x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n            x[i] = x[i].permute(0, 2, 3, 1).contiguous()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"yolov5",src:r(7147).A+"",width:"702",height:"617"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:["After cloning the code, run the script at ",(0,s.jsx)(n.a,{href:"https://github.com/ultralytics/yolov5/blob/v2.0/models/export.py",children:"https://github.com/ultralytics/yolov5/blob/v2.0/models/export.py"})," to convert the .pt files to ONNX."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note"}),": When using the export.py script, consider the following:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Set the ",(0,s.jsx)(n.code,{children:"opset_version"})," parameter in ",(0,s.jsx)(n.code,{children:"torch.onnx.export"})," according to the ONNX opset version you intend to use."]}),"\n",(0,s.jsxs)(n.li,{children:["Adjust the default input name parameters in the ",(0,s.jsx)(n.code,{children:"torch.onnx.export"})," section as needed."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Please note that I have translated the text into English while maintaining the original format and code blocks. I have also made the requested changes to the image file name and data input size."}),"\n",(0,s.jsx)(n.p,{children:'"data"  # Changed "images" to "data" to match the YOLOv5x example script in the model conversion package'}),"\n",(0,s.jsx)(n.h4,{id:"checklist",children:"Model Accuracy Optimization Checklist"}),"\n",(0,s.jsx)(n.p,{children:"Follow the steps 1-5 strictly to verify the model's accuracy, and keep the code and results for each step:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:r(45939).A+"",width:"1864",height:"1016"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Before starting, ensure you have the correct Docker image or conversion environment version, and record the version information."})}),"\n",(0,s.jsx)(n.h5,{id:"1-validate-the-inference-results-of-the-float-onnx-model",children:"1. Validate the inference results of the float ONNX model"}),"\n",(0,s.jsx)(n.p,{children:"Enter the model conversion environment to test the single-image result of the float ONNX model (specifically, the ONNX model exported from the DL framework). This step's result should be identical to the inference result of the trained model (except for possible minor differences due to NV12 format)."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from horizon_tc_ui import HB_ONNXRuntime\nimport numpy as np\nimport cv2\n\ndef preprocess(input_name):\n    # RGB conversion, Resize, CenterCrop...    \n    # HWC to CHW\n    # normalization\n    return data\n\ndef main():\n    # Load the model file\n    sess = HB_ONNXRuntime(model_file=MODEL_PATH)\n    # Get input and output node names\n    input_names = [input.name for input in sess.get_inputs()]\n    output_names = [output.name for output in sess.get_outputs()]\n    # Prepare model input data\n    feed_dict = {input_name: preprocess(input_name) for input_name in input_names}\n    \n    # Original float ONNX, data dtype=float32\n    outputs = sess.run(output_names, feed_dict, input_offset=0)     \n    # Postprocessing\n    postprocess(outputs)\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h5,{id:"2-verify-the-correctness-of-the-yaml-configuration-file-and-preprocessingpostprocessing-code",children:"2. Verify the correctness of the YAML configuration file and preprocessing/postprocessing code"}),"\n",(0,s.jsxs)(n.p,{children:["Test the single-image result of the ",(0,s.jsx)(n.code,{children:"original_float.onnx"})," model, which should be consistent with the float ONNX model's inference result (excluding possible minor differences due to NV12 data loss)."]}),"\n",(0,s.jsxs)(n.p,{children:["Use an open-source tool like Netron to inspect the ",(0,s.jsx)(n.code,{children:"original_float.onnx"}),' model, and examine the detailed properties of the "HzPreprocess" operator to obtain the required parameters for our preprocessing: ',(0,s.jsx)(n.code,{children:"data_format"})," and ",(0,s.jsx)(n.code,{children:"input_type"}),"."]}),"\n",(0,s.jsx)(n.p,{children:'Since the HzPreprocess operator is present, the preprocessing in the converted model might differ from the original. This operator is added based on the configuration parameters in the YAML file (input_type_rt, input_type_train, norm_type, mean_value, scale_value). For more details, refer to the section on "norm_type configuration parameter explanation" in the PTQ principles and steps guide. The preprocessing node will appear in all conversion outputs.'}),"\n",(0,s.jsxs)(n.p,{children:["Ideally, the HzPreprocess node should complete the full type conversion from input_type_rt to input_type_train, but this process is done on the D-Robotics AI chip hardware, which is not included in the ONNX model. Thus, the actual input type in the ONNX model uses a middle type representing the hardware's handling of input_type_rt. For image models with inputs like RGB/BGR/NV12/YUV444/GRAY and dtype=uint8, apply ",(0,s.jsx)(n.code,{children:"-128"})," in the preprocessing when using the ",(0,s.jsx)(n.code,{children:"hb_session.run"})," interface; for featuremap models with dtype=float32, no ",(0,s.jsx)(n.code,{children:"-128"})," is needed, as the input layout (NCHW/NHWC) remains the same as the original float model."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"\nfrom horizon_tc_ui import HB_ONNXRuntime\nimport numpy as np\nimport cv2\n\ndef preprocess(input_name):\n    # BGR to RGB, Resize, CenterCrop...\n    # HWC to CHW (determined by the specific shape of the input node in the ONNX model)\n    # normalization (skip if normalization operation is already included in the model's YAML file)\n    # -128 (apply to image inputs when using the hb_session.run interface; use input_offset for other interfaces)\n    return data\n\ndef main():\n    # Load the model file\n    sess = HB_ONNXRuntime(model_file=MODEL_PATH)\n    # Get input and output node names\n    input_names = [input.name for input in sess.get_inputs()]\n    output_names = [output.name for output in sess.get_outputs()]\n    # Prepare model input data\n    feed_dict = {}\n    for input_name in input_names:\n        feed_dict[input_name] = preprocess(input_name)\n    # Image input models (RGB/BGR/NV12/YUV444/GRAY), data type = uint8\n    outputs = sess.run(output_names, feed_dict, input_offset=128)   \n    # Feature map models, data type = float32. Comment out this line if the model input is not a feature map!\n    # outputs = sess.run_feature(output_names, feed_dict, input_offset=0)   \n    # Post-processing\n    postprocess(outputs)\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h5,{id:"3-validate-that-no-accuracy-loss-was-introduced-during-graph-optimization",children:"3. Validate that no accuracy loss was introduced during graph optimization"}),"\n",(0,s.jsxs)(n.p,{children:["Test the single-image result of the ",(0,s.jsx)(n.code,{children:"optimize_float.onnx"})," model, which should be identical to the ",(0,s.jsx)(n.code,{children:"original_float.onnx"})," inference result."]}),"\n",(0,s.jsxs)(n.p,{children:["Use Netron to inspect the ",(0,s.jsx)(n.code,{children:"optimize_float.onnx"}),' model and check the "HzPreprocess" operator\'s details for the required preprocessing parameters: ',(0,s.jsx)(n.code,{children:"data_format"})," and ",(0,s.jsx)(n.code,{children:"input_type"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from horizon_tc_ui import HB_ONNXRuntime\nimport numpy as np\nimport cv2\n\ndef preprocess(input_name):\n    # BGR to RGB, Resize, CenterCrop...\n    # HWC to CHW (determined by the specific shape of the input node in the ONNX model)\n    # normalization (if normalization operation is already included in the model's YAML file, skip it here)\n    # -128 (apply -128 to image inputs when using the hb_session.run interface; other interfaces can control with input_offset)\n    return data\n\ndef main():\n    # Load the model file\n    sess = HB_ONNXRuntime(model_file=MODEL_PATH)\n    # Get input and output node names\n    input_names = [input.name for input in sess.get_inputs()]\n    output_names = [output.name for output in sess.get_outputs()]\n    # Prepare model input data\n    feed_dict = {input_name: preprocess(input_name) for input_name in input_names}\n    # Image input models (RGB/BGR/NV12/YUV444/GRAY), data dtype= uint8\n    outputs = sess.run(output_names, feed_dict, input_offset=128)         \n    # Feature map models, data dtype=float32. Comment out this line if the model does not take feature maps as input!\n    # outputs = sess.run_feature(output_names, feed_dict, input_offset=0)     \n    # Post-processing\n    postprocess(outputs)\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h5,{id:"4-verify-the-quantization-accuracy-meets-expectations",children:"4. Verify the quantization accuracy meets expectations"}),"\n",(0,s.jsxs)(n.p,{children:["Test the precision metrics of the ",(0,s.jsx)(n.code,{children:"quantized.onnx"})," model."]}),"\n",(0,s.jsxs)(n.p,{children:["Use Netron to open the ",(0,s.jsx)(n.code,{children:"quantized.onnx"}),' model and examine the "HzPreprocess" operator\'s details for the needed preprocessing parameters: ',(0,s.jsx)(n.code,{children:"data_format"})," and ",(0,s.jsx)(n.code,{children:"input_type"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from horizon_tc_ui import HB_ONNXRuntime\nimport numpy as np\nimport cv2\n\ndef preprocess(input_name):\n    # BGR to RGB, Resize, CenterCrop...\n    # HWC to CHW (determine layout conversion based on the specific shape of the input node in the ONNX model)\n    # normalization (skip if normalization operation is already included in the model's YAML file)\n    # -128 (apply -128 to image inputs when using the hb_session.run interface; other interfaces use input_offset)\n    return data\n\ndef main():\n    # Load the model file\n    sess = HB_ONNXRuntime(model_file=MODEL_PATH)\n    # Get input and output node names\n    input_names = [input.name for input in sess.get_inputs()]\n    output_names = [output.name for output in sess.get_outputs()]\n    # Prepare model input data\n    feed_dict = {input_name: preprocess(input_name) for input_name in input_names}\n    # Image input models (RGB/BGR/NV12/YUV444/GRAY), data dtype= uint8\n    outputs = sess.run(output_names, feed_dict, input_offset=128) \n    # Feature map model, data dtype=float32. Comment out the following line if the model input is not a feature map!\n    # outputs = sess.run_feature(output_names, feed_dict, input_offset=0) \n    # Post-processing\n    postprocess(outputs)\n\nif __name__ == '__main__':\n    main()\n\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Verifying Model Compilation and Code Correctness"})}),"\n",(0,s.jsxs)(n.p,{children:["Use the ",(0,s.jsx)(n.code,{children:"hb_model_verifier"})," tool to ensure ../07_Advanced_development between the quantized.onnx and .bin files, with model outputs aligned to at least two or three decimal places."]}),"\n",(0,s.jsxs)(n.p,{children:["For detailed instructions on using ",(0,s.jsx)(n.code,{children:"hb_model_verifier"}),', please refer to the section on PTQ principles and steps in the "hb_model_verifier tool" content.']}),"\n",(0,s.jsx)(n.p,{children:"If the model ../07_Advanced_development check passes, carefully examine the board-side preprocessing and post-processing code!"}),"\n",(0,s.jsx)(n.p,{children:"In case of a failure in the ../07_Advanced_development check between the quantized.onnx and .bin models, contact D-Robotics technical support."}),"\n",(0,s.jsx)(n.h4,{id:"quantization-yaml-configuration-file-templates",children:"Quantization YAML Configuration File Templates"}),"\n",(0,s.jsx)(n.h5,{id:"rdk_x3_caffe_yaml_template",children:"RDK X3 Caffe Model Quantization YAML Template"}),"\n",(0,s.jsxs)(n.p,{children:["Create a ",(0,s.jsx)(n.code,{children:"caffe_config.yaml"})," file and copy the following content, then fill in the marked ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"required parameters"})}),' to proceed with model conversion. For more information on parameter usage, see the "',(0,s.jsx)(n.a,{href:"../07_Advanced_development/04_toolchain_development/intermediate/ptq_process#yaml_config",children:"YAML Configuration File Explanation"}),'" chapter.']}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# Copyright (c) 2020 D-Robotics.All Rights Reserved.\n\n# Parameters related to model conversion\nmodel_parameters:\n\n  # Required parameters\n  # Float-point Caffe network data model file, e.g., caffe_model: './horizon_x3_caffe.caffemodel'\n  caffe_model: ''\n\n  # Required parameters\n  # Caffe network description file, e.g., prototxt: './horizon_x3_caffe.prototxt'\n  prototxt: ''\n\n  march: \"bernoulli2\"\n  layer_out_dump: False\n  working_dir: 'model_output'\n  output_model_file_prefix: 'horizon_x3'\n\n# Input parameters related to the model\ninput_parameters:\n\n  input_name: \"\"\n  input_shape: ''\n  input_type_rt: 'nv12'\n  input_layout_rt: ''\n\n  # Required parameters\n  # Data type used in the original float model training framework, options: rgb/bgr/gray/featuremap/yuv444, e.g., input_type_train: 'bgr'\n  input_type_train: ''\n\n  # Required parameters\n  # Data layout used in the original float model training framework, options: NHWC/NCHW, e.g., input_layout_train: 'NHWC'\n  input_layout_train: ''\n\n  #input_batch: 1\n\n  # Required parameter\n  # Preprocessing method used in the original float model training framework, options: no_preprocess/data_mean/data_scale/data_mean_and_scale\n  # no_preprocess: No operation; mean_value or scale_value do not need to be configured\n  # data_mean: Subtract channel mean (mean_value); comment out scale_value\n  # data_scale: Multiply image pixels by scale_value; comment out mean_value\n  # data_mean_and_scale: Subtract channel mean and then multiply by scale_value; both mean_value and scale_value must be configured\n  norm_type: ''\n\n  # Required parameter\n  # Image mean value to subtract, separated by spaces if channel-wise, e.g., mean_value: 128.0 or mean_value: 111.0 109.0 118.0\n  mean_value: \n\n  # Required parameter\n  # Image scaling factor; separate by spaces if channel-wise, e.g., scale_value: 0.0078125 or scale_value: 0.0078125 0.001215 0.003680\n\n# Parameters related to model quantization\ncalibration_parameters:\n\n  # Required parameter\n  # Directory containing reference images for model calibration, supporting formats like JPEG, BMP. These images should be from a test set, covering diverse scenarios, not extreme conditions like overexposure, saturation, blur, pure black, or pure white.\n  # Configure according to the folder path in the 02_preprocess.sh script, e.g., cal_data_dir: './calibration_data_yuv_f32'\n  cal_data_dir: ''\n\n  cal_data_type: 'float32'\n  calibration_type: 'default'\n  # max_percentile: 0.99996\n\n# Compiler-related parameters\ncompiler_parameters:\n\n  compile_mode: 'latency'\n  debug: False\n  # core_num: 2\n  optimize_level: 'O3'\n"})}),"\n",(0,s.jsx)(n.h5,{id:"rdk_x3_onnx_yaml_template",children:"RDK X3 ONNX Model Quantization YAML Template"}),"\n",(0,s.jsxs)(n.p,{children:["Create a ",(0,s.jsx)(n.code,{children:"onnx_config.yaml"})," file and copy the following content, then fill in the marked ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"required parameters"})}),' to proceed with model conversion. For more information on parameter usage, see the "',(0,s.jsx)(n.a,{href:"../../07_Advanced_development/04_toolchain_development/intermediate/ptq_process#yaml_config",children:"YAML Configuration File Explanation"}),'" chapter.']}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# Copyright (c) 2020 D-Robotics.All Rights Reserved.\n\n# Parameters related to model conversion\nmodel_parameters:\n\n  # Required parameters\n  # Float-point ONNX network data model file, e.g., onnx_model: './horizon_x3_onnx.onnx'\n  onnx_model: ''\n\n  march: \"bernoulli2\"\n  layer_out_dump: False\n  working_dir: 'model_output'\n  output_model_file_prefix: 'horizon_x3'\n\n# Input parameters related to the model\ninput_parameters:\n\n  input_name: \"\"\n  input_shape: ''\n  input_type_rt: 'nv12'\n  input_layout_rt: ''\n\n  # Required parameters\n  # Data type used in the original float model training framework, options: rgb/bgr/gray/featuremap/yuv444, e.g., input_type_train: 'bgr'\n  input_type_train: ''\n\n  # Required parameters\n  # Data layout used in the original float model training framework, options: NHWC/NCHW, e.g., input_layout_train: 'NHWC'\n  input_layout_train: ''\n\n  #input_batch: 1\n\n  # Required parameter\n  # Preprocessing method used in the original float model training framework, options: no_preprocess/data_mean/data_scale/data_mean_and_scale\n  # no_preprocess: No operation; mean_value or scale_value do not need to be configured\n  # data_mean: Subtract channel mean (mean_value); comment out scale_value\n  # data_scale: Multiply image pixels by scale_value; comment out mean_value\n  # data_mean_and_scale: Subtract channel mean and then multiply by scale_value; both mean_value and scale_value must be configured\n  norm_type: ''\n\n  # Required parameter\n  # Image mean value to subtract, separated by spaces if channel-wise, e.g., mean_value: 128.0 or mean_value: 111.0 109.0 118.0\n  mean_value: \n\n  # Required parameter\n  # Image scaling factor; separate by spaces if channel-wise, e.g., scale_value: 0.0078125 or scale_value: 0.0078125 0.001215 0.003680\n"})})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},45939:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/model_accuracy_check-ab33e5e3175f49090adbb3ee38a74f8a.png"},7147:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/yolov5-23ead39653f22b278e25f24289fe3561.png"},28453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>a});var s=r(96540);const o={},t=s.createContext(o);function i(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);